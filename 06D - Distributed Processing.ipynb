{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Processing\n",
    "\n",
    "## Connect to Your Workspace\n",
    "\n",
    "The first thing you need to do is to connect to your workspace using the Azure ML SDK.\n",
    "\n",
    "> **Note**: If the authenticated session with your Azure subscription has expired since you completed the previous exercise, you'll be prompted to reauthenticate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "auth = InteractiveLoginAuthentication(tenant_id='<tenant_id>')\n",
    "ws = Workspace.from_config(path='.', auth=auth)\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an experiment script\n",
    "This is the case when we have a dataset with one single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "folder_name = 'diabetes-distributed-processing'\n",
    "experiment_folder = './' + folder_name\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Copy the data file into the experiment folder\n",
    "#shutil.copy('data/diabetes.csv', os.path.join(folder_name, \"diabetes.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $folder_name/diabetes_data_preparation.py\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Get the experiment run context\n",
    "    run = Run.get_context()\n",
    "\n",
    "    # load the diabetes dataset\n",
    "    data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "    # [0    to 18.5 ) = Low       => 0\n",
    "    # [18.5 to 25.0 ) = Normal    => 1\n",
    "    # [25.0 to 28.0 ) = Increased => 2\n",
    "    # [28.0 to 32.0 ) = High      => 3\n",
    "    # [32.0 to 100.0) = Very high => 4\n",
    "    data['BMILevel'] = pd.cut(data['BMI'], right=False, \n",
    "                              bins=[0, 18.5, 25, 28, 32, 100], \n",
    "                              labels=[0, 1, 2, 3, 4])\n",
    "\n",
    "    # Save a sample of the data in the outputs folder (which gets uploaded automatically)\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    data.to_csv(\"outputs/data.csv\", index=False, header=True)\n",
    "\n",
    "    # Complete the run\n",
    "    run.complete()\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from azureml.core import Experiment, ScriptRunConfig\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(\n",
    "    source_directory=experiment_folder, \n",
    "    script='diabetes_data_preparation.py'\n",
    ")\n",
    "\n",
    "# submit the experiment\n",
    "experiment = Experiment(workspace = ws, name = 'diabetes-experiment-distributed-processing')\n",
    "run = experiment.submit(config=script_config)\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Run for File Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for an experiment\n",
    "\n",
    "Read data from a single file, split into chunks and save them as separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_folder_name = 'data-multiple-files'\n",
    "os.makedirs(data_folder_name, exist_ok=True)\n",
    "\n",
    "# read a file with all data\n",
    "df = pd.read_csv('./data/diabetes.csv')\n",
    "\n",
    "# split data into chunks and save them as separate files\n",
    "size = 100\n",
    "list_of_dfs = [df.loc[i:i+size-1,:] for i in range(0, len(df),size)]\n",
    "for idx, df in enumerate(list_of_dfs):\n",
    "    df.to_csv(f'{data_folder_name}/diabetes_{idx}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register File dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "if 'diabetes multi dataset' not in ws.datasets:\n",
    "    default_ds.upload(\n",
    "        src_dir=data_folder_name,  # Upload all csv files in the folder\n",
    "        target_path='diabetes-multi-data/', # Put it in a folder path in the datastore\n",
    "        overwrite=True,  # Replace existing files of the same name\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "\n",
    "    #Create a tabular dataset from the path on the datastore (this may take a short while)\n",
    "    file_data_set = Dataset.File.from_files(path=(default_ds, 'diabetes-multi-data/*.csv'))\n",
    "\n",
    "    # Register the file dataset\n",
    "    try:\n",
    "        file_data_set = file_data_set.register(workspace=ws, \n",
    "                                name='diabetes multi dataset',\n",
    "                                description='diabetes data split into multiple files',\n",
    "                                tags = {'format':'CSV'},\n",
    "                                create_new_version=True)\n",
    "        print('Dataset registered.')\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "else:\n",
    "    print('Dataset already registered.')\n",
    "    file_data_set = Dataset.get_by_name(ws, name='diabetes multi dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files that are in the File Dataset\n",
    "file_data_set.to_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create compute cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"cpu-comp-cluster\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D11_V2', max_nodes=4)\n",
    "        compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        compute_target.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pipeline for batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $folder_name/diabetes_batch_data_preparation.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from azureml.core import Model\n",
    "\n",
    "\n",
    "def init():\n",
    "    print(\"Executing init() function...\")\n",
    "\n",
    "\n",
    "def run(mini_batch):\n",
    "    print(\"Executing run() function...\")\n",
    "    print(f'dataprep start: {__file__}, run({mini_batch})')\n",
    "    \n",
    "    # empty dataframe that will collect processing result of the entire mini batch.\n",
    "    df_result = pd.DataFrame()\n",
    "\n",
    "    # process each file in the batch\n",
    "    for f in mini_batch:\n",
    "        print(\"f: \", f)\n",
    "\n",
    "        # load the diabetes dataset\n",
    "        data = pd.read_csv(f)\n",
    "\n",
    "        # [0    to 18.5 ) = Low       => 0\n",
    "        # [18.5 to 25.0 ) = Normal    => 1\n",
    "        # [25.0 to 28.0 ) = Increased => 2\n",
    "        # [28.0 to 32.0 ) = High      => 3\n",
    "        # [32.0 to 100.0) = Very high => 4\n",
    "        data['BMILevel'] = pd.cut(data['BMI'], right=False, \n",
    "                                  bins=[0, 18.5, 25, 28, 32, 100], \n",
    "                                  labels=[0, 1, 2, 3, 4])        \n",
    "        \n",
    "        # append result\n",
    "        df_result = pd.concat([df_result, data])\n",
    "\n",
    "    print(f\"Processed:\\n{df_result}\")\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import CondaDependencies, DEFAULT_CPU_IMAGE\n",
    "\n",
    "batch_env = Environment(name=\"batch_environment\")\n",
    "batch_env.python.conda_dependencies = CondaDependencies.create(\n",
    "    pip_packages=['azureml-defaults', 'pandas']\n",
    ")\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "\n",
    "# create output location for processed data\n",
    "output_dir = PipelineData(name=\"prepared_data\", datastore=ws.get_default_datastore())\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=folder_name,\n",
    "    entry_script='diabetes_batch_data_preparation.py',\n",
    "    mini_batch_size=\"20\", # process max 20 csv files in one run() call\n",
    "    error_threshold=10,  # do not ignore any file failure\n",
    "    output_action=\"append_row\",\n",
    "    environment=batch_env,\n",
    "    compute_target=compute_target,\n",
    "    node_count=2)\n",
    "\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name=\"data-pre-process\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[ file_data_set.as_named_input('diabetes_ds') ],\n",
    "    output=output_dir,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\n",
    "pipeline_run = Experiment(ws, 'diabetes-experiment-distributed-processing').submit(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore output\n",
    "\n",
    "Note: The output has no header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "prep_step = pipeline_run.find_step_run('data-pre-process')[0]\n",
    "prepared_data = prep_step.get_output_data(\"prepared_data\")\n",
    "prepared_data.download(local_path=\"output\", overwrite=True)\n",
    "\n",
    "for root, dirs, files in os.walk(\"output\"):\n",
    "    for file in files:\n",
    "        if file.endswith('parallel_run_step.txt'):\n",
    "            result_file = os.path.join(root,file)\n",
    "            \n",
    "            \n",
    "df = pd.read_csv(result_file, delimiter=\" \", header=None)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Run for Tabular Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
